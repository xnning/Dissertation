\section{Discussion}

\subsection{Combining  Application and Checking Modes}

Although the application mode provides us with alternative design choices in a
bidirectional type system, a checking mode can still be \textit{easily} added.
One motivation for the checking mode would be annotated expressions $[[e:A]]$,
where the type of the expression is known and is therefore used to check
the expression, as in DK.

Consider adding $[[e:A]]$ for introducing the checking mode for the language.
Notice that, since the checking mode is stronger than the application mode, when
entering the checking mode the application context is no longer useful. Instead we
use application subtyping to satisfy the application context requirements. A
possible typing rule for annotated expressions is:
%
\[
  \drule{ap-app-anno}
\]

\noindent Here, $[[e]]$ is checked using its annotation $[[A1]]$, and then we
instantiate $[[A1]]$ to $[[B]]$ using application subtyping with the application context
$[[ss]]$.

Now we can have a rule set of the checking mode for all expressions, much like
those checking rules in DK. For example, one useful rule for abstractions in the
checking mode could be \rref{ap-chk-lam}, where the parameter type $[[A1]]$
serves as the type of $[[x]]$, and typing checks the body with $[[B]]$.
%
\[
  \drule{ap-chk-lam}
\]

Moreover, combined with the information flow, the checking rule for application
checks the function with the full type.
%
\[
  \drule{ap-chk-app}
\]


Note that adding annotated expressions might bring convenience for programmers,
since annotations can be more freely placed in a program. For example,
$[[(\x. x 1) : (int -> int) -> int]]$ becomes valid.
However this does not add any expressive power, since annotated expressions
that are typeable would remain typeable after moving the annotations
to bindings. For example the previous program is equivalent to
$[[(\x:int->int. x 1)]]$.

This discussion is a sketch. We have not defined the corresponding declarative
system nor algorithm. However we believe that the addition of the checking mode
will \textit{not} bring surprises to the meta-theory.

\subsection{Additional Constructs}
\label{sec:AP:pairs}

In this section, we show that the application mode is compatible with other
constructs, by discussing how to add support for pairs in the language. A
similar methodology would apply to other constructs like sum types, data types,
if-then-else expressions and so on.

The introduction rule for pairs must be in the inference mode with an empty
application context. Also, the subtyping rule for pairs is as expected.
%
\begin{mathpar}
  \drule{ap-inf-pair}
  \and
  \drule{ap-s-pair}
\end{mathpar}

The application mode can apply to the elimination constructs of pairs. If one
component of the pair is a function, for example, $[[fst (\x. x, 1) 2]]$, then
it is possible to have a judgment with a non-empty application context.
Therefore, we can use the application subtyping to account for the application
contexts:
%
\begin{mathpar}
  \drule{ap-app-fst} \\ \drule{ap-app-snd}
\end{mathpar}

However, in polymorphic type systems, we need to take the subsumption rule into
consideration. For example, in the expression $[[(\x:\/a. (a, b). fst x)]]$,
$[[fst]]$ is applied to a polymorphic type. Interestingly, instead of a
non-deterministic subsumption rule, having polymorphic types actually leads to a
simpler solution. According to the philosophy of the application mode, the types
of the arguments always flow into the functions. Therefore, instead of regarding
$[[fst e]]$ as an expression form, where $[[e]]$ is itself an argument, we could
regard $[[fst]]$ as a function on its own, whose type is $[[\/ a. \/ b. (a, b) ->
a]]$. Then as in the variable case, we use the subtyping rule to deal with
application contexts. Thus the typing rules for $[[fst]]$ and $[[snd]]$ can be
modeled as:
% 
\begin{mathpar}
  \drule{ap-app-fst-var} \and \drule{ap-app-snd-var}
\end{mathpar}

Note that another way to model those two rules would be to simply have an
initial typing environment $[[dd]]_{init} \equiv [[fst : \/a. \/b. (a, b) ->
a]], [[snd : \/a. \/b. (a, b) -> b]]$. In this case the elimination of pairs be
dealt directly by the rule for variables.

An extended version of the calculus extended with rules for pairs
(\rref{ap-inf-pair}, \rref{ap-s-pair}, \rref{ap-app-fst-var} and
\rref{ap-app-snd-var}), has been formally studied. All the theorems presented
before hold with the extension of pairs.

\subsection{More Expressive Type Applications}
\label{sec:AP:typeapplication}

The design choice of propagating arguments to functions was subject to
consideration in the original work on local type inference
\citep{pierce:local}, but was rejected due to possible non-determinism
introduced by explicit type applications:

\begin{quote}
  \emph{``It is possible, of course, to come up with examples where it would be
    beneficial to synthesize the argument types first and then use the resulting
    information to avoid type annotations in the function part of an application
    expression....Unfortunately this refinement does not help infer the type of
    polymorphic functions. For example, we cannot uniquely determine the type of
    $x$ in the expression $(\mathbf{fun}[X](x) ~ e) ~ [ [[int]] ] ~
    3$.''}
\end{quote}

As a response to this challenge, we also present an application of the
application mode to a variant of System F \citep{esop2018:arguments}. The
development of the calculus shows that the \mode mode can actually work well
with calculi with explicit type applications. Here we explain the key ideas of
the design of the system, but refer to \cite{esop2018:arguments} for more
details.

To explain the new design, consider the expression:
%
\[
  (\Lambda [[a]]. [[\x :a . x + 1]])~[[int]]
\]

\noindent which is not typeable in the traditional type system for System F. In
System F the lambda abstractions do not account for the context of possible
function applications. Therefore when type checking the inner body of the lambda
abstraction, the expression $[[x+1]]$ is ill-typed, because all that is known is
that $[[x]]$ has the (abstract) type $[[a]]$.

If we are allowed to propagate type information from arguments to functions,
then we can verify that $[[a = int]]$ and $[[x+1]]$ is well-typed. The key
insight in the new type system is to use contexts to track type equalities
induced by type applications. This enables us to type check expressions such as
the body of the lambda above ($[[x+1]]$). The key rules for type abstractions
and type applications are:
%
\begin{mathpar}
  \inferrule*[right=ap-app-tapp]{
    [[dd]] ; [[ss]], [ [ [[dd]] ] [[A1]] ] [[|-AP]] [[e]] [[=>]] [[B]]
  }{
    [[dd]] ; [[ss]] [[|-AP]] [[e]]~[[A1]] [[=>]] [[B]]
  }
\and
  \inferrule*[right=ap-app-tlam]{
    [[dd]], [[a = A1]]; [[ss]] [[|-AP]] [[e]] [[=>]] [[B]]
  }{
    [[dd]] ; [[ss]], [ [[A1]] ] [[|-AP]] [[/\]] [[a]] . [[e]] [[=>]] [[B]]
  }
\end{mathpar}
%
For type applications, \rref{ap-app-tapp} stores the type argument $[[A1]]$ into
the application context. Since $[[dd]]$ tracks type equalities, we apply
$[[dd]]$ as a type substitution to $[[A1]]$ (i.e., $[ [[dd]] ] [[A1]]$)
Moreover, to distinguish between type arguments and types of term arguments, we
put type arguments in brackets (i.e., $[ [ [[dd]] ] [[A1]] ]$). For type
abstractions (\rref{ap-app-tlam}), if the application context is non-empty, we
put a new type equality between the type variable $[[a]]$ and the type argument
$[[A1]]$ into the context.

Now, back to the problematic expression $(\mathbf{fun}[X](x) ~ e) ~ [
[[int]] ] ~ 3$, the type of $[[x]]$ can be inferred as either $X$ or $[[int]]$
since they are actually equivalent.

\paragraph{Sugar for Type Synonyms.}

In the same way that we can regard $[[let]]$ expressions as syntactic sugar, in
the new type system we further \textit{gain built-in type synonyms}. A
\textit{type synonym} is a new name for an existing type. Type synonyms are
common in languages such as Haskell. In our calculus a simple form of type
synonyms can be desugared as follows:
%
\[
\mathbf{type}~[[a = A]]~\mathbf{in}~[[e]] [[~~>]] (\Lambda [[a]]. [[e]]) [[A]]
\]
%
One practical benefit of such syntactic sugar is that it enables a direct
encoding of a System F-like language with declarations (including
type-synonyms). Although declarations are often viewed as a routine extension to
a calculus, and are not formally studied, they are highly relevant in practice.
Therefore, a more realistic formalization of a programming language should
directly account for declarations. By providing a way to encode declarations,
our new calculus enables a simple way to formalize declarations.

\paragraph{Type Abstraction.}

The type equalities introduced by type applications may seem like we are
breaking System F type abstraction. However, we argue that \textit{type
  abstraction} is still supported by our System F variant. For example:
%
\[
  \mathbf{let}~ inc = \Lambda a. [[\x:a. x + 1]]  ~\mathbf{in}~ inc~ [[int]]~[[1]]
\]
%
\noindent (after desugaring) does \textit{not} type-check, as in a System-F like
language. In our type system lambda abstractions that are immediatelly applied
to an argument, and unapplied lambda abstractions behave differently. Unapplied
lambda abstractions are just like System F abstractions and retain type
abstraction. The example above illustrates this. In contrast the typeable
example $(\Lambda [[a]]. [[\x:a. x + 1]])~[[int]]$, which uses a lambda
abstraction directly applied to an argument, can be regarded as the desugared
expression for $\mathbf{type}~[[a = int]]~\mathbf{in}~[[\x:a. x + 1]]$.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Thesis"
%%% org-ref-default-bibliography: "../../Thesis.bib"
%%% End: