\section{Type Inference for Higher-Rank Types}


\citet{odersky1996putting} introduced a type system for higher-rank implicit
polymorphic types. Based on that, \citet{practical:inference} developed an
approach for type inference for higher-rank types using traditional
bidirectional type checking. They combine a variant of the containment relation
from \citet{mitchell1988polymorphic} for deep skolemisation in subsumption
rules, which we believe is compatible with our subtyping definition.

\citet{DK} build a simple and concise algorithm for higher-rank polymorphism
based on traditional bidirectional type checking. They deal with the same
language of \citet{practical:inference}, except they do not have $[[let]]$
expressions nor generalization (though it is discussed in design variations).
Built upon some of these techniques, \cite{DK:extend} extend the system to a
much richer type language that includes existentials, indexed types, and
equations over type variables.

\newcommand{\mlf}{\mathit{ML^F}}

While our work focuses on predicative higher-rank types, there are also a lot of
work on type inference for \textit{impredicative} higher-rank types. Many of
these work replies on new forms of types. $\mlf$
\citep{le2014mlf,remy2008graphic,le2009recasting} generalizes ML with
first-class polymorphism. $\mlf$ introduces a new type of bounded quantification
(either rigid or flexible) for polymorphic types so that instantiation of
polymorphic bindings is delayed until a principal type is found. higher-rank
types. The HML system \citep{leijen2009flexible} is proposed as a simplification
and restriction of $\mlf$. HML only uses flexible types, which simplifies the
type inference algorithm, but retains many interesting properties and features.

The FPH system \citep{vytiniotis2008fph} introduces boxy monotypes into System F
types. One critique of boxy type inference is that the impredicativity is deeply
hidden in the algorithmic type inference rules, which makes it hard to
understand the interaction between its predicative constraints and impredicative
instantiations~\citep{remy2005simple}.

Recently, \citet{guarded:impred,quick:look} exploit impredicative instantiations
of type variables that appears under a type constructor (i.e., type variables
are \textit{guarded}). \cite{guarded:impred} distinguish variables using three
\textit{sorts},
% which specify the possible types that the variable can stand for,
so that certain sorts of variables can be instantiated with higher-rank
polymorphic types. \cite{quick:look} inspect the function arguments and assign
impredicative instantiations before monomorphic ones.

\section{Bidirectional Type Checking}


Bidirectional type checking was popularized by the work of
\citet{pierce:local}. It has since been applied to many type systems with
advanced features. The alternative \mode mode introduced in
\Cref{chap:BiDirectional} enables a variant of bidirectional type checking.
There are many other efforts to refine bidirectional type checking.

Colored local type inference \citep{odersky2001colored} allows partial type
information to be propagated, by distinguishing inherited
types (known from the context) and synthesized types (inferred from terms). A
similar distinction is achieved in \cite{DK} by manipulating type variables.

\emph{Tridirectional} type checking
\citep{dunfield:tridirectional} is based on bidirectional type checking and has
a rich set of property types including intersections, unions and quantified
dependent types, but without parametric polymorphism. Tridirectional type
checking has a new direction for supporting type checking unions and existential
quantification.

Greedy bidirectional polymorphism \citep{dunfield2009greedy} adopts a greedy
idea from \citet{cardelli1993implementation} on bidirectional type checking
with higher-rank types, where type variables in instantiations are
determined by their first constraint. In this way, they support some uses of
impredicative polymorphism. However, the greediness also makes many obvious
programs rejected.

A detailed survey of the development of bidirectional type checking is given by
\cite{dunfield2020bidirectional}, which collect and explain the design
principles of bidirectional type checking, and summarize past research related
to bidirectional type checking.


\section{Gradual Typing}

The seminal paper by \citet{siek2006gradual} is the first to propose gradual
typing, which enables programmers to mix static and dynamic typing in a program
by providing a mechanism to control which parts of a program are statically
checked. The original proposal extends the simply typed lambda calculus by
introducing the unknown type $\unknown$ and replacing type equality with type
consistency. Casts are introduced to mediate between statically and dynamically
typed code. Later \citet{siek2007gradual} incorporated gradual typing into a
simple object oriented language, and showed that subtyping and consistency are
orthogonal -- an insight that partly inspired our work on \gpc. We show that subtyping
and consistency are orthogonal in a much richer type system with higher-rank
polymorphism. \citet{siek2009exploring} explores the design space of different
dynamic semantics for simply typed lambda calculus with casts and unknown types.
In the light of the ever-growing popularity of gradual typing, and its somewhat
murky theoretical foundations, \citet{siek:criteria} felt the urge to have a
complete formal characterization of what it means to be gradually typed. They
proposed a set of criteria that provides important guidelines for designers of
gradually typed languages. \citet{cimini2016gradualizer} introduced the
\emph{Gradualizer}, a general methodology for generating gradual type systems
from static type systems. Later they also develop an algorithm to generate
dynamic semantics~\citep{CiminiPOPL}. \citet{garcia:abstracting} introduced
the AGT approach based on abstract interpretation. As we discussed, none of
these approaches instructed us how to define consistent subtyping for
polymorphic types.

There is some work on integrating gradual typing with rich type disciplines.
\citet{Ba_ados_Schwerter_2014} establish a framework to combine gradual typing and
effects, with which a static effect system can be transformed to a dynamic
effect system or any intermediate blend. \citet{Jafery:2017:SUR:3093333.3009865}
present a type system with \emph{gradual sums}, which combines refinement and
imprecision. We have discussed the interesting definition of \emph{directed
  consistency} in \Cref{sec:gradual:exploration}. \citet{castagna2017gradual} develop a gradual type system with
intersection and union types, with consistent subtyping defined by following
the idea of \citet{garcia:abstracting}.
\citet{eremondi:gradualdependent} develop a gradual dependently-typed language,
where compile-time normalization and run-time execution are distinguished to
account for nontermination and failure.
TypeScript~\citep{typescript} has a distinguished dynamic type, written {\color{blue} any}, whose fundamental feature is that any type can be
implicitly converted to and from {\color{blue} any}.
% They prove that the conversion
% definition (called \emph{assignment compatibility}) coincides with the
% restriction operator from \citet{siek2007gradual}.
Our treatment of the unknown type in \Cref{fig:gradual:decl:conssub} is similar to their
treatment of {\color{blue} any}. However, their type system does not have
polymorphic types. Also, unlike our consistent subtyping which inserts runtime
casts, in TypeScript, type information is erased after compilation so there are
no runtime casts, which makes runtime type errors possible.
% dynamic checks does not contribute to type safety.

\section{Gradual Type Systems with Explicit Polymorphism}

\citet{Morris:1973:TS:512927.512938} dynamically enforces
parametric polymorphism and uses \emph{sealing} functions as the
dynamic type mechanism. More recent works on integrating gradual typing with
parametric polymorphism include the dynamic type of \citet{abadi1995dynamic} and
the \emph{Sage} language of \citet{gronski2006sage}. None of these has carefully
studied the interaction between statically and dynamically typed code.

\citet{amal:blame} proposed \pbc that extends the blame
calculus~\citep{Wadler_2009} to incorporate polymorphism. The key novelty of
their work is to use dynamic sealing to enforce parametricity. As such, they end
up with a sophisticated dynamic semantics. Later, \citet{amal2017blame} prove
that with more restrictions, \pbc satisfies parametricity. Compared to their
work, our \gpc type system can catch more errors earlier since, as we argued,
their notion of \emph{compatibility} is too permissive. For example, the
following is rejected (more precisely, the corresponding source program never
gets elaborated) by our type system:

\[
  [[(\x:unknown. x + 1)]] : [[\/ a. a -> a]] [[~~>]] [[ <unknown -> nat `-> \/a. a -> a> (\x:unknown. x + 1) ]]
\]
while the type system of \pbc would accept the translation, though at runtime,
the program would result in a cast error as it violates parametricity.
% This does not imply, in any regard that \pbc is not well-designed; there are
% circumstances where runtime checks are \emph{needed} to ensure
% parametricity.
We emphasize that it is the combination of our powerful type system together
with the powerful dynamic semantics of \pbc that makes it possible to have
implicit higher-rank polymorphism in a gradually typed setting.
% without compromising parametricity.
\citet{devriese2017parametricity} proved that
embedding of System F terms into \pbc is not fully abstract. \citet{yuu2017poly}
also studied integrating gradual typing with parametric polymorphism. They
proposed System F$_G$, a gradually typed extension of System F, and System
F$_C$, a new polymorphic blame calculus. As has been discussed extensively,
their definition of type consistency does not apply to our setting (implicit
polymorphism). All of these approaches mix consistency with subtyping to some
extent, which we argue should be orthogonal. On a side note, it seems that our
calculus can also be safely translated to System F$_C$. However we do not
understand all the tradeoffs involved in the choice between \pbc and System
F$_C$ as a target.

Recently, \citet{toro:gradual:parametricity} applied AGT to designing a gradual
language with explicit parametric polymorphism, claiming that graduality and
parametricity are inherently incompatible. However, later
\citet{new:gradual:parametricity} show that by modifying System F's syntax to
make the sealing visible, both graduality and parametricity can be achieved.

\section{Gradual Type Inference}

\citet{siek2008gradual} studied unification-based type inference for gradual
typing, where they show why three straightforward approaches fail to meet their
design goals. One of their main observations is
that simply ignoring dynamic types during unification does not work. Therefore,
their type system assigns unknown types to type variables and infers gradual
types, which results in a complicated type system and inference algorithm. In
our algorithm presented in \cref{chap:Dynamic}, comparisons between
existential variables and unknown types are emphasized by the distinction
between static existential variables and gradual existential variables. By
syntactically refining unsolved gradual existential variables with unknown types, we gain a
similar effect as assigning unknown types, while keeping the algorithm relatively
simple.
\citet{garcia:principal} presented a new approach where gradual type
inference only produces static types, which is adopted in our type system. They
also deal with let-polymorphism (rank 1 types). They proposed the distinction
between static and gradual type parameters, which inspired our extension to
restore the dynamic gradual guarantee. Although those existing works all involve
gradual types and inference, none of these works deal with higher-rank
implicit polymorphism.



\section{Kind Inference for Datatypes}

\paragraph{The Glasgow Haskell Compiler.}

The systems we present here are inspired by the algorithms
implemented in GHC. However, our goal in the design of these systems is to produce
a sound and (nearly) complete pair of specification and implementation, not simply
to faithfully record what is implemented. We have identified ways that the GHC
implementation can improve in the future. For example, GHC quantifies over local scopes
as \emph{specified} where
we believe they should be \emph{inferred}; and the tight connection in our system
between unification and promotion may improve upon GHC's approach, which separates
the two. The details
of the relationship between our work and GHC (including a myriad of ways our design
choices differ in small ways from GHC's) appear in \cite{xie2019kind}.

\paragraph{Unification with dependent types.}

While full higher-order unification is
undecidable~\citep{goldfarb1981undecidability}, the \textit{pattern}
fragment~\cite{miller1991unification} is a well-known decidable fragment. Much 
 literature~\citep{reed2009higher,abel2011higher, gundry2013tutorial} is
built upon the pattern fragment.

Unification in a dependently typed language features \emph{heterogeneous
  constraints}. To prove correctness, \citet{reed2009higher} used a weaker
invariant on homogeneous equality, \textit{typing modulo}, which states that two
sides are well typed up to the equality of the constraint yet to be solved.
\citet{gundry2013tutorial} observed the same problem, and use \textit{twin
  variables} to explicitly represent the same variable at different types, where
twin variables are eliminated once the heterogeneous constraint is solved. In
both approaches the well-formedness of a constraint depends on other
constraints.
\citet{Cockx:2016:UEP:2951913.2951917} proposed a proof-relevant unification that
keeps track of the dependencies between equations.
Different from their approaches, our algorithm unifies the kinds when
solving unification variables. This guarantees that
our unification always outputs well-formed solutions.

\citet{ziliani2015unification} present the higher-order unification algorithm
for CIC, the base logic of Coq. They favor syntactic equality by trying
first-order unification, as they argue the first-order solution gives the most
\textit{natural} solution. However, they omit a correctness proof for their algorithm.
\citet{coen2004mathematical} also considers first-order unification,
but only the soundness lemma is proved. Different from their systems, our system
is based on the novel promotion judgment, and correctness including
soundness and termination is proved.
 
The technique of \textit{suspended
  substitutions}~\cite{eisenberg2016dependent, gundry2013tutorial} is widely
adopted in unification algorithms. Our
system provides a design alternative, our \textit{quantification check}.
Choosing between suspended substitutions and the quantification check is a
user-facing language design decision, as suspended substitutions can accept some
more
programs. The quantification check means that the kind of a locally quantified
variable |a| must be fully determined in |a|'s scope; it may \emph{not} be
influenced by usage sites of the construct that depends on |a|. Suspended
substitutions relax this restriction.
%% With suspended substitutions, a unification variable $[[Xa]]$ may have a
%% telescope of variables that $[[Xa]]$ can depend on. Recall the example |T| in
%% \Cref{subsec:qcheck}, we have $|c|::[[Xb]]$, where $[[Xb]] |:: a|$, but we
%% cannot generalize $[[Xb]]$, so |T| is rejected. Using suspended substitutions, we
%% can keep $|c::| [[ Xb ]] [ |a: *, b : a | ] $, where $ [ | a:star, b : a | ] $
%% is the telescope of $[[Xb]]$. Later, if we have |P T| (with |P| defined below),
%% then $[[Xb]]$ can be solved to |b|.
%% \begin{spec}
%% data P :: (forall (a :: *) (b :: a) (c :: b). Relate b c -> *) -> *
%% \end{spec}
We conjecture that suspended substitutions can yield a complete algorithm.
However, that mechanism is complex. Moreover,
unification based on suspended substitutions is only decidable for the pattern
fragment. Our system, in contrast, avoids all the complication
introduced by suspended substitutions through its quantification check.
Our unification terminates for all inputs, preserving
backward compatibility to Hindley-Milner-style inference. Although
we reject the definition of |T| (\Cref{sec:kind:qcheck}), we can solve more
constraints outside
the pattern fragment. We conjecture that those constraints are much
more common than definitions like |T|.
Suspended substitutions often come with a \textit{pruning}
process~\citep{abel2011higher},
which produces a
valid solution before solving a unification variable.
Our promotion process has a similar effect.

\paragraph{Homogeneous kind-preserving unification}

\citet{jones1995system} proposed a homogeneous kind-preserving unification
between two types. Kinds $[[k]]$ are defined only as
$[[star]]$ or $[[k1 -> k2]]$. As the kind system is much simpler,
kind-preserving unification $[[~]]_{[[k]]}$ is simply subscripted by the kind,
and working out the kinds is straightforward. Our unification subsumes Jones's algorithm. 

% \paragraph{Datatypes}

% General-purpose dependently typed languages, including Coq~\citep{coqteam},
% Agda~\citep{norell2007towards}, and Idris~\citep{brady2013idris}, can define
% datatypes in a similar way to Haskell datatypes. However, as far as we know,
% there is no formalization established of their inference algorithm for
% datatypes. \ningning{not sure whether to include the following;} We conjecture
% that their underlying inference mechanism for datatypes is different from
% our system. For example, the unification algorithm in
% Coq~\citep{ziliani2015unification} is quite different from ours.
% \richard{I don't find that this paragraph contributes much. Cut?}
% \ningning{Cut for now.}

\paragraph{Type inference in Haskell}

Type inference in Haskell is inspired by
\citet{damas1982principal} and \citet{pottier2005essence},
extended with various
type features, including
higher rank polymorphism~\citep{practical:inference} and
local
assumptions~\citep{simonet2007constraint,Schrijvers:2009:CDT:1596550.1596599,
  vytiniotis2011outsidein},
among
others.
However, none of these works describe an inference algorithm
for datatypes, nor do they formalize type variables of varying kinds or polymorphic recursion.

\paragraph{Dependent Haskell}
Our \tit system merges types and kinds, a key feature of
\textit{Dependent
  Haskell} (DH)~\citep{eisenberg2016dependent,gundry2013type,weirich2017specification,
  Weirich:2013:SFE:2500365.2500599}. There is ongoing work dedicated to its
implementation~\citep{coercionq}. The most recent work by
\citet{weirich2019role} integrates \textit{roles}~\cite{breitner2016safe}
with dependent types.
Our work is the first presentation of unification for DH,
and our system
may be useful in designing DH's
term-level type inference.

\paragraph{Context extension}

TODO
Our approach of recording unification variables and their solutions in the
contexts is inspired by \citet{gundry2010type} and \citet{dunfield2013complete}.
\citet{gundry2013tutorial} applied the
approach to unification in dependent types, where the context also records
constraints; constraints also appear in context in \citet{eisenberg2016dependent}.
Further, we extend
the context extension approach with local scopes, supporting 
groups of order-insensitive variables.

\paragraph{Polymorphic recursion}

\citet{mycroft1984polymorphic} presented a semi-algorithm for polymorphic
recursion. \citet{jim1996principal} and \citet{damiani2003rank} studied typing rules for
recursive definitions based on rank-2 intersection types.
\citet{comini2008polymorphic} studied recursive definitions in a type system
that corresponds to the abstract interpreter in
\citet{gori2002experiment,gori2003properties}. Our system does not infer
polymorphic recursion; instead, we exploit kind annotations to guide the acceptance of polymorphic
recursion.

% \paragraph{Constraint-solving approaches}

% Many systems (e.g.~\citep{pottier2005essence}) adopt
% a modular presentation of type inference, which consists of a constraint
% generator and a constraint solver. For simplicity, we have presented an eager
% unification algorithm instead of using a separate constraint solver. However, we
% believe changing to a constraint-solving approach should not change any of our
% main results. Section~B.1 of \auxiliarymaterial considers this point further.



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Thesis"
%%% org-ref-default-bibliography: "../../Thesis.bib"
%%% End: