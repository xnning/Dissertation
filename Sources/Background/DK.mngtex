\section{The Dunfield-Krishnaswami Type System}
\label{sec:DK}

Both HM and OL derive only monotypes for unannotated lambda abstractions. OL
improves on HM by allowing polymorphic lambda abstractions but requires the
polymorphic type annotations to be given explicitly. The Dunfield-Krishnaswami
type system \citep{DK}, hereafter refered to as DK, give a
\textit{bidirectional} account of higher-rank polymorphism, where type
information can be propagated through the syntax tree. Therefore, it is possible
for a variable bound in a lambda abstraction without explicit type annotations
to get a polymorphic type. In this section, we first review the idea of
bidirectional type checking, and then present the declarative DK and discuss its
algorithm.



\subsection{Bidirectional Type Checking}
\label{sec:DK:bidirectional}

Bidirectional type checking has been known in the folklore of type systems for
a long time. It was popularized by Pierce and Turner's work on local type
inference \citep{pierce:local}. Local type inference was introduced as an
alternative to HM type systems, which could easily deal with polymorphic
languages with subtyping. The key idea in local type inference is simple.
The "local" in local type inference comes from the fact that:

\begin{quote}
\textit{
  ``... missing annotations are recovered using
  only information from adjacent nodes in the syntax tree, without long-distance
  constraints such as unification variables.''}
\end{quote}


Bidirectional type checking is one component of local type inference that, aided
by some type annotations, enables type inference in an expressive language with
polymorphism and subtyping. In its basic form typing is split into
\textit{inference} and \textit{checking} modes. The most salient feature of a
bidirectional type-checker is when information deduced from inference mode is
used to guide checking of an expression in checking mode.

Since Pierce and Turner's work, various other authors have proved the
effectiveness of bidirectional type checking in several other settings,
including many different systems with subtyping
\citep{davies:intersection,dunfield:tridirectional}, systems with dependent
types \citep{asperti:bi:dependent,coquand:algorithm,loh:tutorial,xi:dependent},
etc.

In particular, bidirectional type checking has also been combined with HM-style
techniques for providing type inference in the presence of higher-rank type,
including DK and \cite{practical:inference}. Let's revisit the example in
\Cref{sec:OL}:

\begin{lstlisting}
(\f. (f 1, f 'a')) (\x. x)
\end{lstlisting}

\noindent which is not typeable in HM as it they fail to infer the type of
\lstinline{f}. In OL, it can be type-checked by adding a polymorphic type
annotation on \lstinline{f}. In DK, we can also add a polymorphic type
annotation on \lstinline{f}. But with bidirectional type checking, the type
annotation can be propagated from somewhere else. For example, we can rewrite
this program as:

\begin{lstlisting}
((\f. (f 1, f 'c')) : (foralla. a -> a) -> (Int, Char)) (\x . x)
\end{lstlisting}

\noindent Here the type of \lstinline{f} can be easily derived from the type
signature using checking mode in bidirectional type checking.

\cite{dunfield:tridirectional} establish a design principle of bidirectional
type checking inspired by \textit{mode correctness} from logical programming,
where \textit{introduction rules} are distinguished from \textit{elimination
  rules}. Following the design principle, constructors corresponding to
introduction rules (e.g., tuples) are checked against a given type, while
destructors corresponding to elimination rules (e.g., tuple projections) infer a
type. DK is designed following this principle.


\begin{figure}[t]
  \centering
    \begin{tabular}{lrcl} \toprule
      Expressions & $[[e]]$ & \syndef & $[[x]] \mid [[n]] \mid [[\x . e]]  \mid [[e1 e2]] \mid [[e : A]] $ \\
      Types & $[[A]] $ & \syndef & $ [[int]] \mid [[a]] \mid  [[A1 -> A2]] \mid [[\/ a. A]] $ \\
      Monotypes & $[[t]]$ & \syndef & $ [[int]] \mid [[a]] \mid [[t1 -> t2]] $ \\
      Contexts & $[[dd]]$ & \syndef & $[[empty]] \mid [[dd , x : A]] \mid [[dd , a]] $ \\
      \bottomrule
    \end{tabular} \\
  \caption{Syntax of the Dunfield-Krishnaswami Type System}
  \label{fig:DK}
\end{figure}

\subsection{Declarative System}
\label{sec:DK:declarative}

\paragraph{Syntax.}

The syntax of the DK is given in \Cref{fig:DK}. Comparing to OL, only the
definition of expressions slightly differs. First, the expressions $[[e]]$ in DK
have no let expressions. \cite{DK} omitted let-bindings from the formal
development, but argued that restoring let-bindings is easy, as long as they get
no special treatment incompatible with substitution (e.g., a syntax-directed HM
does polymorphic generalization only at let-bindings). Second, DK has annotated
expressions $[[e:A]]$ (instead of annotated lambda expressions $[[\x:A. e]]$),
in which the type annotation can be propagated into the
expression, as we will see shortly.

The definitions of types and contexts are the same as in OL. Thus, DK also shares
the same well-formedness definition as in OL (\Cref{fig:OL:wf}). We thus omit the
definitions, but use $[[dd |-DK A]]$ to denote the corresponding judgment in DK.

\paragraph{Type System.}

\begin{figure}
  \drules[dk-inf]{$ [[dd |-DK e => A ]] $}{Type Inference}{var, int,lam, app, anno}
  \drules[dk-chk]{$ [[dd |-DK e <= A ]] $}{Type Checking}{int, lam, gen, sub}
  \drules[dk-app]{$ [[dd |-DK A1 . e =>=> A2]] $}{Application judgment}{forall,arr}
  \caption{Static semantics of the Dunfield-Krishnaswami type system.}
  \label{fig:DK:static}
\end{figure}

\Cref{fig:DK:static} presents the typing rules for DK. The system uses
bidirectional type checking to accommodate polymorphism. Traditionally, two
modes are employed in bidirectional systems: the inference mode $[[dd |-DK e =>
A]]$, which takes a term $[[e]]$ and produces a type $[[A]]$, similar to the
judgment $[[dd |-HM e : A]]$ or $[[dd |-OL e : A]]$ in previous systems; the
checking mode $[[dd |-DK e <= A]]$, which takes a term $[[e]]$ and a type
$[[A]]$ as input, and ensures that the term $[[e]]$ checks against $[[A]]$. We
first discuss rules in the inference mode.

\paragraph{Type Inference.}

\Rref{dk-inf-var} and \rref{dk-inf-int} are straightforward. To infer
unannotated lambdas, \rref{dk-inf-lam} guesses a monotype. For an application
$[[e1 e2]]$, \rref{dk-inf-app} first infers the type $[[A]]$ of the expression
$[[e1]]$. The \textit{application judgment} (discussed shortly) then takes the
type $[[A]]$ and the argument $[[e2]]$, and returns the final result type
$[[A2]]$. For an annotated expression $[[e : A]]$, \rref{dk-inf-anno} simply
checks $[[e]]$ against $[[A]]$. Both rules (\rref{dk-inf-app} and
\rref{dk-inf-anno}) have mode switched from inference to checking.

\paragraph{Type Checking.}

Now we turn to the checking mode. When an expression is checked against a type,
the expression is expected to have that type. More importantly, the checking mode
allows us to push the type information into the expressions.

\Rref{dk-chk-int} checks literals against the integer type $[[int]]$.
\Rref{dk-chk-lam} is where the system benefits from bidirectional type checking:
the type information gets pushed inside an lambda. For an unannotated lambda
abstraction $[[\x. e]]$, recall that in the inference mode, we can only guess a
monotype for $[[x]]$. With the checking mode, when $[[\x. e]]$ is checked
against $[[A1 -> A2]]$, we do not need to guess any type. Instead, $[[x]]$ gets
directly the (possibly polymorphic) argument type $[[A1]]$. Then the rule
proceeds by checking $[[e]]$ with $[[A2]]$, allowing the type information to be
pushed further inside. Note how \rref{dk-chk-lam} improves over HM and OL, by
allowing lambda abstractions to have a polymorphic argument type without
requiring type annotations.

\Rref{dk-chk-gen} deals with a polymorphic type $[[\/a. A]]$, by putting the
(fresh) type variable $[[a]]$ into the context to check $[[e]]$ against $[[A]]$.
\Rref{dk-chk-sub} switches the mode from checking to inference: an expression
$[[e]]$ can be checked against $[[A2]]$, if $[[e]]$ infers the type $[[A1]]$ and
$[[A1]]$ is a subtype of $[[A2]]$.

\paragraph{Application judgment.}

Notably, unlike HM or OL, DK does not feature an explicit instantiation rule.
Instead, \rref{dk-inf-var} directly returns a (possibly) polymorphic type, and
thus when typing applications (\rref{dk-app-inf}), we need to explicitly discuss
the possible shape of the function type.

The application judgment $[[dd |-DK A1 . e =>=> A2]]$ is interpreted as, when we
apply an expression of type $[[A1]]$ to the expression $[[e]]$, we get a return
type $[[A2]]$. For a polymorphic type (\rref{dk-app-forall}), we
instantiate the universal quantifier with a monotype, until the type becomes a
function type (\rref{dk-app-arr}). In the function type case, since the function
expects an argument of type $[[A1]]$, the rule proceeds by checking $[[e2]]$
against $[[A1]]$.

In some other type systems
\citep{garcia:principal,esop2018:consistent,toplas:consistent}, the application
judgment is replaced by \textit{matching}. Using matching, \rref{dk-inf-app} is
replaced by \rref{dk-inf-app2}.

\begin{center}
  \drule{dk-inf-appTwo}
\end{center}

\noindent
In \rref{dk-inf-app2}, we first derive that $[[e1]]$ has type $[[A]]$. But
$[[e1]]$ must have a function type so that it can be applied to an argument. We
thus use the \textit{matching} judgment to instantiate $[[A]]$ into a function
$[[A1 -> A2]]$, and proceed by checking $[[e2]]$ against $[[A1]]$, and return
the final result $[[A2]]$.
The definition of matching is given below.

\drules[dk-m]{$ [[dd |-DK A1 |> A2]] $}{Matching}{forall, arr}

Matching has two straightforward rules: \rref{dk-m-forall} instantiates a
polymorphic type, by substituting $[[a]]$ with a well-formed monotype $[[t]]$,
and continues matching on $[[ A[a ~> t] ]]$; \rref{dk-m-arr} returns the
function type directly.

It can be easily shown that the presentation of \rref{dk-inf-app} with the
application judgment is equivalent to that of \rref{dk-inf-app2} with matching.
Essentially, they both make sure that the expression being applied has an arrow
type $[[A1 -> A2]]$, and then check the argument against $[[A1]]$. We sometimes
use the presentation of \rref{dk-inf-app2} with matching, as matching is a
simple and independent process whose purpose is clear. In contrast, it is
relatively less comprehensible with \rref{dk-inf-app} and the application
judgment, where all three forms of the judgment (inference, checking,
application) are mutually dependent.

\paragraph{Subtyping.} DK shares the same subtyping relation as of OL. We thus
omit the definition and use $[[dd |-DK A1 <: A2]]$ to denote the subtyping
relation in DK.


\subsection{Algorithmic Type System}

\cite{DK} also presented a sound and complete bidirectional algorithmic type
system. The key idea of the algorithm is using \textit{ordered} algorithmic
contexts for storing existential variables and their solutions. Comparing to the
algorithm for HM, they argued that their algorithm is remarkably simple. The
algorithm is later discussed and used in \Cref{part:gradual} and
\Cref{part:promotion}. We will discuss more about it there.

\subsection{Discussion: lazy and eager instantiation}

We say that DK's style of instantiation is \textit{lazy}, where top-level quantifiers
are only instantiated when needed (e.g., when applied as a function to
arguments); while HM's style of instantiation is \textit{eager}, where in the
syntax-directed rules, instantiation eagerly instantiates all top-level universal
quantifiers when possible (as in \rref{hm-var-inst}). Eager instantiation is also
used in the higher-rank polymorphic type system in \cite{practical:inference}.

The differences between lazy and eager instantiation have pervasive
consequences. The first and direct consequence is that for the same expression
lazy instantiation can derive a more polymorphic type. Consider that given
\lstinline{id: foralla. a -> a}, we want to
infer the type of \lstinline{id}. With lazy instantiation, the algorithm can
directly return \lstinline{foralla. a -> a} unchanged, while eager instantiation
can only return \lstinline{a -> a} (among others). In this case, we can make two
types match by generalizing the final result from eager instantiation, which
gives us \lstinline{foralla. a -> a}. However, when the system features
higher-rank types, after generalization eager instantiation may still fail
to derive the same type.
For example, consider the expression
%
\begin{lstlisting}
  \x: Int. id
\end{lstlisting}
%
\noindent In this case, lazy instantion returns the type
\lstinline{Int -> foralla. a -> a}, while eager instantiation returns the type
\lstinline{Int -> a -> a} which, after generalization,
becomes \lstinline{foralla. Int -> a -> a}, and is less polymorphic than
\lstinline{Int -> foralla. a -> a}, according to the subtyping relation in OL.
In this sense naive eager instantiation may reject programs that lazy
instantiation can accept,
e.g.,
%
\begin{lstlisting}
let g = \x:Int. id in (\f: Int -> foralla. a -> a. f) g
\end{lstlisting}
%
The problem can be avoided by, for example, featuring \textit{deep
skolemisation} as in \cite{practical:inference}. Deep skolemisation floats out all its
universal quantifiers that appear to the right of a top level arrow, so that
\lstinline{foralla. Int -> a -> a} and \lstinline{Int -> foralla. a -> a} are
actually isomorphic.

% One possible fix is to use
% \textit{eager generalization}, by generalising every lambda body.
% With eager instantiation and eager generalization, the type of \lstinline{id}
% will be instantiated and then immediately generalized again, so
% \lstinline{\x:Int. id} can return \lstinline{Int -> \foralla. a -> a}

Secondly, with lazy instantiation, as instantiation is done only when necessary,
we must be aware of \textit{when} it is necessary. We have seen one particular
case in the formalism, i.e., when applying a top-level polymorphic type as a
function to arguments (\rref{dk-app-forall}). When we extend the system with
more constructs, there may be more cases where instantiation is needed. For
example, consider we extend the system with \lstinline{if} expressions:
%
\begin{lstlisting}
if flag then id else \x. x + 1
\end{lstlisting}
%
\noindent We expect the two branches in \lstinline{if} to return the same type.
Therefore when collecting the result type from branches, we must instantiate the
type of \lstinline{id} to \lstinline{Int -> Int}, so that it matches the type of
another branch. \cite{practical:inference} discuss three possible typing
formalizations of \lstinline{if} expressions under eager instantiation. To adopt
either formalization in lazy instantiation systems, instantiation of the return
types of the branches must be performed.

Finally, the choice can have unexpected interaction with other rules involving
generalization, for example, \lstinline{let} expressions. Specifically, under
let generalization and lazy instantiation, typability may not be preserved after
let-binding inlining. To illustrate the issue, consider the expression:
%
\begin{lstlisting}
let f = \x. x
in let g = \y. f
in e
\end{lstlisting}
%
If we generalize the type of \lstinline{f} as in HM, then we get
\lstinline{f:foralla. a -> a}. Now we continue type-checking \lstinline{g},
which requires another step of generalization. With lazy instantiation,
we get \lstinline{g: forallb. b -> foralla. a -> a}, which is again, more
polymorphic than the type \lstinline{g: forallb a. b -> a -> a} we get from
eager instantiation. However, if we inline the definition of \lstinline{f}, then
the program becomes
%
\begin{lstlisting}
  in let g = \y. \x. x
  in e
\end{lstlisting}
%
\noindent Now both lazy and eager instantiation can only get the type
\lstinline{g: forallb a. b -> a -> a}. Namely, with lazy instantiation,
let-binding inlining gets us a less polymorphic type (according to the subtyping
rule in OL). The problem has also been discussed in DK \citep{DK}, where they
fix the issue by not treating let expressions specially -- namely, there is no
generalization for let expressions at all.

In this thesis, the choice does not matter so much for the key contributions of
the work. As we do not feature deep skolemisation, we mostly follow the idea of
lazy instantiation as in DK. Nevertheless, we believe that deep skolemisation is
compatible with our subtyping relations used in later chapters.






%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../../Thesis"
%%% org-ref-default-bibliography: "../../Thesis.bib"
%%% End: